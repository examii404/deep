import torch
from torch import nn
from torch.utils.data import DataLoader, TensorDataset

# Create random data and labels
data = torch.randint(0, 1000, (100, 10))
labels = torch.randint(0, 2, (100,))

# Create dataset and dataloader
dataset = TensorDataset(data, labels)
loader = DataLoader(dataset, batch_size=10, shuffle=True)

# Define the LSTM Classifier model
class LSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):
        super(LSTMClassifier, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        x = self.embedding(x)
        _, (hidden, _) = self.lstm(x)
        return self.fc(hidden.squeeze(0))

# Instantiate the model, criterion, and optimizer
model = LSTMClassifier(vocab_size=1000, embedding_dim=50, hidden_dim=100, output_dim=2)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters())

# Training function
def train(n_epochs):
    for epoch in range(n_epochs):
        model.train()
        train_loss = 0.0
        for data, targets in loader:
            outputs = model(data)
            loss = criterion(outputs, targets)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
        avg_train_loss = train_loss / len(loader)
        print(f"Epoch {epoch + 1}, Loss: {avg_train_loss:.4f}")

# Train the model for 10 epochs
train(10)
